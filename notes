
##############
general notes
##############

starting loss should be around log 2 ~= 0.7

reasonable results should happen by 1000 iterations and possibly sooner

in order to speed up startup time, change the shuffle size from 256 to something like 32 in inputs.py

##################
about the camera:
##################

by default, the scene is placed 4 units in front of the camera. this is specified in constants.radius, although i'm not certain that i replaced all instance of "4" with this variable.

the camera azimuth ranges from 0 to 360 in increments of 20, and elevation ranges from 0 to 30 in increments of 10. this is specified in constants.py as well (MINH/MAXH/MINV/MAXV, etc). these are the only two degrees of freedom.

the input image is square, so the vertical fov = horizontal fov = 30 degrees. specified in constants.py

the focal length, in units (not pixels!) is computed from the fov, so it is not necessary to specify it.

the input image size is specified by Hdata and Wdata although the image is resized to H by W before it is passed into the model. (constants.py)

-----

i think that the only place that the camera parameters are being used is in utils/voxel.py.
the camera azim and elevation is used to construct an transformation matrix RT.
the main important function in this file is `transformer`

if we want to transform a voxel from view 1 to view 2, RT maps points in view 2 to points in view 1. this is why some of the rotation matrices involved are kind of inverted.

then, a 3D sampling grid of points (from view 2) is constructed, the points are transformed to view 1 by mulitplying by RT, and then trilinear interpolation is used to differentiably sample at the locations of the points.

-----

note about axes:

there is a transformer_preprocess and transformer_postprocess function in voxel.py
if i remeber correctly, transformer_preprocess fixes and issue with the depth axis being reversed in the input ground truth voxel.
the voxel transformer seems to use the axis order ZYX, whereas we use the axis order YXZ. transformer_preprocess does this transpose.

###############
about the data:
###############

1. the data is rendered using the scripts in "MVnet_active/data/render_scripts"

2. in order to create the voxel data, binvox is used. the precise command line arguments for calling binvox are:
./binvox -aw -dc -down -pb -d 256 -bb -0.5 -0.5 -0.5 0.5 0.5 0.5 -t binvox $1

i typically put this one line command in a bash script and call it using something like
find data_directory -type f -name "*.obj" | xargs -n1 -P8 ./binvox.sh

3. use 3dmapping/setup/gen_tfr_multi.py to generate tfrecords from the rendered data and voxels. might need to modify IN_DIR and OUT_DIR

4. update setup/lists/double_train, setup/lists/double_test, setup/lists/double_val. i don't think there is a script for this

##############
code flow:
##############

1. First, all the inputs are unprojected in the function construct_pred_main_input

2. Next, in predict_, they are all rotated to the pose of the first input view via the function translate_views_multi

3. Then, aggregate_inputs is called in order to combine all the unprojected inputs into a single 3D tensor

4. Finally, voxel_net_3d is called in order to generate the reconstruction
